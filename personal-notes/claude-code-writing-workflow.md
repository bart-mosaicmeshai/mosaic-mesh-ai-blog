# How I Use Claude Code to Write Blog Series

**Status**: Future blog post or series idea
**Created**: Dec 10, 2025
**Category**: Questioning or Learning
**Project**: mosaic-mesh-ai-blog

---

## The Concept

A meta post (or series) about the actual workflow of using Claude Code as a writing partner to create technical blog series. Not theoretical "how AI can help writers"—the actual, messy, iterative process we use.

---

## Why This Would Be Interesting

**Unique angle:**
- Most AI writing content is about "AI replacing writers" or basic "use ChatGPT for brainstorming"
- This is about **collaborative technical writing** with an AI agent
- Shows the real workflow: prompts, iterations, fact-checking, code verification
- Demonstrates AI as a peer, not a replacement

**Authentic evidence:**
- Real session transcripts from Agentic Personal Trainer Parts 6-9
- Actual edits and iterations (Part 6 context window discussion)
- Real bugs discovered (Part 8 callback tracking)
- Code verification against actual repositories
- The nano-banana enhancement discovered mid-session

**Valuable for readers:**
- Developers building AI tools who want to document their work
- Technical writers exploring AI collaboration
- Anyone doing "building in public" with AI assistance

---

## Evidence from Recent Sessions

### Session C (Dec 10, 2025) - Agentic Personal Trainer Parts 6-9

**What happened:**
1. Started with proofreading Part 6
2. Discovered "last 5 conversations" was arbitrary, undocumented
3. Deep dive into context windows (8K vs 1M tokens)
4. Clarified learned_preferences is infrastructure-only
5. Added vector embeddings cross-reference to EmbeddingGemma series
6. Discovered nano-banana workflow friction mid-session
7. Spawned separate agent to fix nano-banana (20 min enhancement)
8. Used new workflow for Parts 7-9 (saved 15 minutes)
9. Fixed Part 8 callback tracking bug with another agent
10. Ran actual tests to get real output for Part 8
11. Created 9-part Gemma/Bluey series drafts

**Key collaboration patterns:**
- **Fact-checking**: Claude verified claims against git history
- **Code cross-referencing**: Read actual code to confirm blog accuracy
- **Consistency enforcement**: Applied learned_preferences pattern across parts
- **Link standardization**: "see [Part X]" pattern across Part 9
- **Real-time problem solving**: Fixed tools while writing about them
- **Meta moments**: Enhancement paid for itself while writing about it

### Session Artifacts

**Documents created/edited:**
- 4 blog posts proofread and edited (Parts 6-9)
- 1 new blog post drafted (Dec 15 nano-banana)
- 9 new series drafts created (Gemma/Bluey)
- 3 personal notes files updated
- README.md updated with workflow
- 5 images generated with new automated workflow

**Technical work completed:**
- nano-banana JPEG automation (separate agent)
- debug-conversation.js callback bug fix (separate agent)
- Asset reorganization

**Commits made:**
- Part 4 + Parts 5-9 images
- Asset reorganization

---

## Potential Post/Series Structures

### Option A: Single Meta Post (Dec 26-27?)
**Title**: "How I Use Claude Code to Write Technical Blog Series"

**Content:**
- The workflow (read → verify → edit → generate images → commit)
- Spawning separate agents for mid-session bugs
- Fact-checking against git history
- Real example: Part 6 context window deep dive
- The nano-banana automation discovery
- ROI: faster writing, more accurate content, better cross-references

**Hook**: "I wrote 9 blog posts about AI with an AI writing partner. Here's the workflow that emerged."

### Option B: 3-Part Mini-Series
**Part 1**: "Writing Technical Content with an AI Agent"
- The basic workflow
- How Claude verifies code claims
- Real example from Part 6

**Part 2**: "When Your Writing Partner Fixes Your Tools"
- The nano-banana discovery mid-session
- Spawning separate agents for bugs
- Using the enhancement while writing about it

**Part 3**: "The Meta Workflow: Lessons from 27 AI Blog Posts"
- What works (fact-checking, cross-references)
- What doesn't (over-relying on AI without verification)
- The human-AI collaboration sweet spot

### Option C: Ongoing "Behind the Scenes" Posts
- Don't make it a series
- Write occasional meta posts about interesting sessions
- "How Claude Caught My Unsupported Claims"
- "The Bug We Fixed While Writing About Testing"
- "Spawning Sub-Agents: AI Agents All The Way Down"

---

## Key Themes to Cover

**The Workflow:**
- Session start prompts and context loading
- Reading code to verify blog claims
- Iterative editing based on what we find
- Cross-referencing between series
- Image generation with nano-banana
- Git commits with Claude

**The Collaboration:**
- Claude catches unsupported claims ("dozens of prompt variations")
- Claude reads actual code to verify line numbers
- Claude spots inconsistencies across parts
- Human provides direction, AI provides thoroughness
- Real-time course correction

**The Tools:**
- Claude Code CLI and its capabilities
- Spawning separate agents for parallel work
- The Read/Edit/Bash tool patterns
- nano-banana integration
- Git workflow

**The Meta Moments:**
- Building tools while writing about them
- Enhancement paying for itself generating its own blog image
- Testing tools to write about testing
- AI writing about AI development

**The Challenges:**
- Keeping AI grounded in facts (git history verification)
- Balancing speed with accuracy
- When to trust AI vs when to verify
- Managing context window across long sessions

---

## What Makes This Different

**Not another "I used ChatGPT to write" post:**
- This is about technical accuracy, not content generation
- Shows the verification process, not just output
- Demonstrates specialized tools (Claude Code CLI), not generic chat
- Covers code verification, git history checking, spawning sub-agents
- Real examples of catching and fixing inaccuracies

**The honest parts:**
- Times Claude was wrong (unsupported claims)
- When verification is critical (code references, git history)
- The friction points (manual image workflow before automation)
- What humans still do better (judgment calls, direction)

---

## Timing Considerations

**Pros of writing this soon:**
- Fresh memory of the session details
- Can reference recent posts (Agentic Personal Trainer, nano-banana)
- Demonstrates real value of the workflow

**Cons of writing this soon:**
- Might want more session examples first
- Could benefit from patterns across multiple series
- Risk seeming too "meta" too quickly

**Recommendation:** Wait until after Gemma/Bluey series complete AND Claude Code series is well underway. Then you'll have substantial evidence:
1. Agentic Personal Trainer (9 parts) - COMPLETE
2. Nano-banana enhancement (1 part) - COMPLETE
3. Gemma/Bluey fine-tuning (8 parts) - COMPLETE (Jan 2026)
4. Bart Test (5 parts) - Launching Jan 5-14, 2026
5. Claude Code series (48-52 parts) - Launching Jan 17, 2026
6. Merchant series (20 parts) - Launching Jan 31, 2026 (monthly)

That's 90+ posts written with this workflow—extremely strong evidence base.

**Timing:** Write this meta-series around mid-2026 when you have 6+ months of Claude Code collaboration documented. This becomes the "capstone" series showing how all the other series were written.

---

## Session Evidence to Capture

**For this specific session (Dec 10):**
- Total time: ~4-5 hours
- Posts processed: 4 (Parts 6-9)
- New posts created: 1 (Dec 15) + 9 (Gemma series)
- Technical bugs fixed: 2 (nano-banana workflow, callback tracking)
- Commits made: 2
- Images generated: 4 (Parts 7-9, Dec 15)
- Separate agents spawned: 2
- Real code verified: Multiple files across 2 repos

---

## Next Actions

- **Save this note** for future reference
- **After Gemma series starts**: Revisit and decide on format
- **Collect more session examples**: Document interesting workflows as they happen
- **Build the case**: Show this workflow produces better, more accurate content faster

---

This could become signature content—showing how AI collaboration actually works in technical writing, not the sanitized marketing version.
